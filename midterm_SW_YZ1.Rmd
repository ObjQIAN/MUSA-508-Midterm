---
title: "MUSA 508, Lab 4 - Spatial Machine Learning Pt. 1"
author: "Harris, Fichman, and Steif - 2022/23"
output: html_document
---

# 1 Introduction

In response to the increasing demand for a housing market model tailored to the unique characteristics of Philadelphia, we have embarked on a study aimed at enhancing the precision and applicability of home sale price predictions. Our goal is to develop a robust model that takes into account both internal and external factors influencing housing prices in the city.

Employing an Ordinary Least Squares (OLS) Linear regression model, we have integrated data from various sources to create a comprehensive framework. This encompasses information on the intrinsic attributes of homes, graciously provided by our client, as well as external data derived from Philadelphia Open Data and the American Community Survey conducted by the U.S. Census Bureau. This combination of internal and external data is crucial in striving for a model that accurately reflects the dynamic Philadelphia housing market.

This report delineates our methodology and approach, underscoring the paramount importance of considering local context in tandem with internal characteristics. While our model is a work in progress, we are confident that it will furnish us with valuable insights into the factors that influence homebuyers' preferences and, consequently, shape the evolving landscape of the Philadelphia housing market.

# 2 Data Manipulation and Visualization

## 2.0 Set up

In this section, we initiated the process by loading essential libraries, establishing plot theme configurations, and defining map theme settings. Additionally, we identified and prepared functions for quintile breaks and calculating the average nearest neighbor distance, setting the stage for a more in-depth analysis.

```{r setup, include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidyverse)
library(tidycensus)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(broom)
library(tufte)
library(rmarkdown)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")
```

```{r read_data}

district <- 
  st_read("https://raw.githubusercontent.com/ObjQIAN/MUSA-508-Midterm/main/data/Planning_Districts.geojson") %>%
  dplyr::select(DIST_NAME,ABBREV) %>% #Select data for later prediction
  st_transform('ESRI:102729')

nhoods <- 
  st_read("D:/Upenn/23fall/MUSA5080 Public Policy Analysis/midterm 4-5/studentData.geojson") %>%
  st_transform('ESRI:102729')

to_predict <-
  nhoods %>%
  dplyr::filter(toPredict == 'CHALLENGE') #Select data for later prediction

to_train <-
  nhoods %>%
  dplyr::filter(toPredict == 'MODELLING') #Select data for training Model

#??
#Philadelphia <- 
 # read.csv(file.path(root.dir,"/Chapter3_4/phillyHousePriceData_clean.csv"))

```


## 2.1 Data Wrangling

### 2.1.1 Data loading

Here are the data we will use in our study:

-   House Price and Internal Characteristics: basic geometric dataset provided by the course in advance.

-   Exterial Characteristics:

  -   1. Census Data: demographic variables from the ACS 2020 for census tracts in Miami-Dade County and we selected the features we are interested in below:

    -   `TotalPop`: ASC total population estimated in each census tract
    -   `Whites`: People describing themselves as "white alone" in each census tract
    -   `VacantHU`: Vacant house units in each census tract
    -   `TotalHU`: Estimate of total housing units in each census tract
    -   `FemalBachelors`: Female bachelors in each census tract
    -   `MaleBachelors`: Male bachelors in each census tract
    -   `MedHHInc`: Median household income ($) in each census tract
    -   `MedRent`: Median Rent for properties in each census tract
    -   `TotalPoverty`: Population living under the level of poverty in each census tract
              
  And based on the features above, we calculated several new features which are more intuitive. 

    -   `pctWhite`: White population proportion in each census tract
    -   `pctVacant`: Vacant house unit proportion in each census tract
    -   `pctBachelors`: Bachelor population proportion in each census tract
    -   `pctPoverty`: Poverty population proportion in each census tract
  
  -   2. Crime Data:

  -   3. Amenity Data:
    
    -   `Crime`:
    -   `311`:
    -   `Landmark`:
    -   ``:
    

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("5c7b1ebb206012789759942ddf1acbb882f937ad", overwrite = TRUE)
```

```{r}
acsTractsPHL.2020 <- 
  get_acs(geography = "tract", 
          variables = c("B25026_001E","B02001_002E",
                        "B25002_003E", "B25002_001E",
                        "B15001_050E","B15001_009E",
                        "B19013_001E","B25058_001E",
                        "B06012_002E"), 
          year=2020, state=42, county=101, 
          geometry=TRUE, output="wide") %>%
  st_transform('ESRI:102729') %>%
  rename(TotalPop = B25026_001E, 
         Whites = B02001_002E,
         VacantHU = B25002_003E,
         TotalHU = B25002_001E,
         FemaleBachelors = B15001_050E, 
         MaleBachelors = B15001_009E,
         MedHHInc = B19013_001E, 
         MedRent = B25058_001E,
         TotalPoverty = B06012_002E) %>%
  dplyr::select(-NAME, -starts_with("B")) %>%
  mutate(pctWhite = ifelse(TotalPop > 0, Whites / TotalPop,0),
         pctVacant = ifelse(TotalPop > 0, VacantHU / TotalPop,0),
         pctBachelors = ifelse(TotalPop > 0, ((FemaleBachelors + MaleBachelors) / TotalPop),0),
         pctPoverty = ifelse(TotalPop > 0, TotalPoverty / TotalPop, 0),
         year = "2020") %>%
  dplyr::select(-Whites, -FemaleBachelors, -MaleBachelors, -TotalPoverty) 

```

```{r acs_vars}
acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B25002_001E", # Estimate of total housing units
              "B25002_003E", # Number of vacant housing units
              "B19013_001E", # Median HH Income ($)
              "B02001_002E", # People describing themselves as "white alone"
              "B06009_006E") # Total graduate or professional degree
```

```{r get_acs_2020, cache = TRUE, message = FALSE, warning = FALSE}

acsTractsPHL.2020 <- get_acs(geography = "tract",
                             year = 2020, 
                             variables = acs_vars, 
                             geometry = TRUE, 
                             state = "PA", 
                             county = "Philadelphia", 
                             output = "wide") %>%
                            st_transform('ESRI:102729')

```

```{r price_map}

ggplot() +
  geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  geom_sf(data = to_train, aes(colour = q5(sale_price)), 
          show.legend = "point", size = .35) +
  scale_colour_manual(values = palette5,
                   labels=qBr(to_train,"sale_price"),
                   name="Quintile\nBreaks") +
  labs(title="House Sale Price, Philadelphia") +
  mapTheme()

```

```{r}

# Load crime data
philadelphiCrimes <- read.csv('https://raw.githubusercontent.com/ObjQIAN/MUSA-508-Midterm/main/data/Philadelphia_crime.csv') 

# Create sf and select Weapon Violations
philadelphiCrimes.sf <-
  philadelphiCrimes %>%
  filter(text_general_code == "Weapon Violations",
  lat > -1) %>%
  dplyr::select(lat, lng) %>%
  na.omit() %>%
  st_as_sf(coords = c( "lng","lat"), crs = "EPSG:4326") %>%
  st_transform('ESRI:102729') %>%
  distinct()

# Load 311 data
philadelphia311.sf <- read.csv('https://phl.carto.com/api/v2/sql?filename=public_cases_fc&format=csv&skipfields=cartodb_id,the_geom,the_geom_webmercator&q=SELECT%20*%20FROM%20public_cases_fc%20WHERE%20requested_datetime%20%3E=%20%272022-01-01%27%20AND%20requested_datetime%20%3C%20%272023-01-01%27') %>%
  dplyr::select(lat, lon) %>%
  na.omit() %>%
  st_as_sf(coords = c("lon", "lat"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102729')



```


### 2.1.2 Feature Engineering

#### (1) Crime Data Set

```{r Features}

# Counts of crime per buffer of house sale
to_train$crimes.Buffer <- to_train %>% 
    st_buffer(660) %>% 
    aggregate(mutate(philadelphiCrimes.sf, counter = 1),., sum) %>%
    pull(counter)


## Nearest Neighbor Feature

to_train <-
  to_train %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphiCrimes.sf), k = 5)) 
```

```{r assault density}
## Plot assault density
ggplot() + geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(philadelphiCrimes.sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = "none") +
  labs(title = "Density of WA, PHL") +
  mapTheme()
```

#### (2) 311 Data Set

```{r Features}

# Counts of 311 per buffer of house sale
to_train$philly311.Buffer <- to_train %>% 
    st_buffer(660) %>% 
    aggregate(mutate(philadelphia311.sf, counter = 1),., sum) %>%
    pull(counter)


## Nearest Neighbor Feature

to_train <-
  to_train %>% 
    mutate(
      p311_nn1 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 1),
      
      p311_nn2 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 2), 
      
      p311_nn3 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 3), 
      
      p311_nn4 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 4), 
      
      p311_nn5 = nn_function(st_coordinates(to_train), 
                              st_coordinates(philadelphia311.sf), k = 5)) 
```

```{r 311 density}
## Plot 311 density-????
ggplot() + geom_sf(data = acsTractsPHL.2020, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(philadelphia311.sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.1, 0.35), guide = "none") +
  labs(title = "Density of 311, PHL") +
  mapTheme()
```

# 3 Analyzing Associations

Run these code blocks...
Notice the use of `st_drop_geometry()`, this is the correct way to go from a `sf` spatial dataframe to a regular dataframe with no spatial component.

Can somebody walk me through what they do?

Can you give me a one-sentence description of what the takeaway is?

## 3.1 Sale Price as a Function of Numeric Features

```{r}

## Crime cor
to_train %>%
  st_drop_geometry() %>%
  mutate(Age = 2022 - year_built) %>%
  dplyr::select(sale_price, starts_with("crime_")) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + 
     geom_smooth(data = . %>% filter(sale_price > 0), method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()

```
The result shown in the plot above is too similar, so we calculate and compare the R^2 to choose which crime data to use.
The result shows that crime_nn3 is the best one (Adjusted R-squared:  0.1118).
```{r}
# the result above is too similar, So we calculate and compare the R^2 to choose which crime data to use.
# The result shows that crime_nn3 is the best one.
to_train <- to_train %>%
  mutate(Age = 2022 - year_built) 

philly_sub_200k <- st_drop_geometry(to_train) %>% 
filter(sale_price <= 2000000, total_livable_area < 10000, total_livable_area > 0) 

Crime1Reg <- lm(sale_price ~ crime_nn1, data = philly_sub_200k)
summary(Crime1Reg)

Crime2Reg <- lm(sale_price ~ crime_nn2, data = philly_sub_200k)
summary(Crime2Reg)

Crime3Reg <- lm(sale_price ~ crime_nn3, data = philly_sub_200k)
summary(Crime3Reg)

Crime4Reg <- lm(sale_price ~ crime_nn4, data = philly_sub_200k)
summary(Crime4Reg)

Crime5Reg <- lm(sale_price ~ crime_nn5, data = philly_sub_200k)
summary(Crime5Reg)

```

We use the scatter plot to initially determine whether the following Numeric Features are suitable as our variables. As can be seen from the chart below, some of the features that may be useful are: `Age`, `crime_nn3`, `depth`, `frontage`, `philly311.Buffer`, `total_livable_area`.
(若结果拟合度不足可以考虑继续添加)

```{r Correlation}

## Home Features cor

st_drop_geometry(to_train) %>% 
  dplyr::select(sale_price, total_livable_area, Age, crime_nn3, 
                depth, frontage, off_street_open, philly311.Buffer) %>%
  filter(sale_price <= 1000000, Age < 500, total_livable_area <10000, depth < 600, frontage < 500) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + 
    geom_smooth(data = . %>% filter(sale_price >0), method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()

```

## 3.2 Sale Price as a Function of Categorical Features

We use the histograms to initially determine whether the following categorical features are suitable as our variables. As can be seen from the chart below, some of the features that may be useful are: `exterior_condition`, `fireplaces`, `interior_condition`, `number_of_bathrooms`, `number_of_bedrooms`, `number_stories`.
(若结果拟合度不足可以考虑继续添加)

```{r}

#这里的代码我不知道要怎么合并一下好

to_train %>%
  dplyr::select(sale_price, exterior_condition) %>%
  mutate(exterior_condition = as.factor(exterior_condition)) %>%
  filter(sale_price <= 1000000) %>%
  group_by(exterior_condition) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = exterior_condition, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, fireplaces) %>%
  mutate(fireplaces = as.factor(fireplaces)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(fireplaces)) %>%
  group_by(fireplaces) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = fireplaces, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, interior_condition) %>%
  mutate(interior_condition = as.factor(interior_condition)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(interior_condition)) %>%
  group_by(interior_condition) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = interior_condition, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, number_of_bathrooms) %>%
  mutate(number_of_bathrooms = as.factor(number_of_bathrooms)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(number_of_bathrooms)) %>%
  group_by(number_of_bathrooms) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = number_of_bathrooms, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, number_of_bedrooms) %>%
  mutate(number_of_bedrooms = as.factor(number_of_bedrooms)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(number_of_bedrooms)) %>%
  group_by(number_of_bedrooms) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = number_of_bedrooms, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

to_train %>%
  dplyr::select(sale_price, number_stories) %>%
  mutate(number_stories = as.factor(number_stories)) %>%
  filter(sale_price <= 1000000) %>%
  filter(!is.na(number_stories)) %>%
  group_by(number_stories) %>%
  summarize(avg_sale_price = mean(sale_price)) %>%
  ggplot(aes(x = number_stories, y = avg_sale_price)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Average Sale Price by Exterior Condition",
      y = "Average Sale Price"
    ) +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## 3.3 Select Variables

In order to prevent the selected features from having too strong correlations between them and affecting the regression results, we use a correlation matrix to provide us with pairwise correlations for each set of features in the data, and use Pearson's r - Correlation Coefficient to Look at the contribution of each variable. 

### 3.3.1  Correlation matrix

The correlation matrix reveals a notable association between `exterior_condition` and `interior_condition`. Upon closer examination, it becomes evident that `interior_condition` offers a more robust explanation for fluctuations in `sale_price` (R-squared for `exterior` = 0.168, R-squared for `interior` = 0.1699). 

Furthermore, `Total_livable_area` demonstrates a substantial correlation with both `frontage` and `depth`. Our choice to include `Total_livable_area` is substantiated by the steepness of the regression line observed in the scatterplot.

To encapsulate, the chosen numeric features encompass `Age`, `crime_nn3`, `philly311.Buffer`, `total_livable_area`, while the selected categorical features comprise `fireplaces`, `interior_condition`, `number_of_bathrooms`, `number_of_bedrooms`, and `number_stories`.

```{r correlation_matrix}
numericVars <- 
  select_if(st_drop_geometry(to_train), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 


#有无穷或遗漏值？
# yet another way to plot the correlation plot using the corrr library
#numericVars %>% 
#  correlate() %>% 
#  autoplot() +
#  geom_text(aes(label = round(r,digits=2)),size = 2)

```
```{r}

ex_Reg <- lm(sale_price ~ exterior_condition, data = philly_sub_200k)
summary(ex_Reg)

in_Reg <- lm(sale_price ~ interior_condition, data = philly_sub_200k)
summary(in_Reg)

```

### 3.3.2 Univarite correlation with Pearson's r - Correlation Coefficient

```{r uni_variate_Regression}

cor.test(philly_sub_200k$total_livable_area,
         philly_sub_200k$sale_price, 
         method = "pearson")

```

## Univarite Regression

### R2 - Coefficient of Determination

```{r simple_reg}
livingReg <- lm(sale_price ~ total_livable_area, data = philly_sub_200k)

summary(livingReg)

ggscatter(philly_sub_200k,
          x = "total_livable_area",
          y = "sale_price",
          add = "reg.line") +
  stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")), label.y = 2500000) +
  stat_regline_equation(label.y = 2250000) 

```


## Prediction example

Make a prediction using the coefficient, intercept etc.,

```{r calculate prediction}
coefficients(livingReg)

new_total_livable_area = 4000

# "by hand"
-41433.9841  + 88.34939 * new_total_livable_area

# predict() function
predict(livingReg, newdata = data.frame(total_livable_area = 4000))

```


# 4 Multivariate OLS Regression


```{r mutlivariate_regression}
reg1 <- lm(sale_price ~ ., data = philly_sub_200k %>% 
                                 dplyr::select(sale_price, Age, total_livable_area, crime_nn3, philly311.Buffer, 
                                               fireplaces, interior_condition, number_of_bathrooms, number_of_bedrooms, number_stories))

summary(reg1)

```

## Marginal Response Plots

Let's try some of these out. They help you learn more about the relationships in the model.

What does a long line on either side of the blue circle suggest?

What does the location of the blue circle relative to the center line at zero suggest?

```{r effect_plots}
## Plot of marginal response
effect_plot(reg1, pred = total_livable_area, interval = TRUE, plot.points = TRUE)

## Plot coefficients
plot_summs(reg1, scale = TRUE)

## plot multiple model coeffs
plot_summs(reg1, livingReg)


```

Challenges:

-What is the Coefficient of total_livable_area when Average Distance to 2-nearest crimes are considered?

-Build a regression with total_livable_area and crime_nn2? Report the regression coefficient for total_livable_area. Is it different than it was before? Why?

- Try to engineer a 'fixed effect' out of the other variables in an attempt to parameterize a variable that suggests a big or fancy house or levels of fanciness. How does this affect your model?

```{r}


```




## Split Data into Train/Test Set
```{r}

inTrain <- createDataPartition(
              y = paste(to_train$building_code_description, to_train$quality_grade), 
              p = .60, list = FALSE)
philly.training <- to_train[inTrain,] 
philly.test <- to_train[-inTrain,]  
 
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                             dplyr::select(sale_price, total_livable_area, crimes.Buffer))

philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(reg.training, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict)%>%
  filter(sale_price < 5000000) 

```

```{r}
# Remove invalid predictions (Maybe need to do this before running the test)
philly.test <-  philly.test[!with(philly.test,is.na(sale_price.Predict)),]
```
## 5 Spatial Lags

What is the relationship between errors? Are they clustered? Is the error of a single observation correlated with the error of nearby observations?

We create a list of "neigbhors" using a "spatial weights matrix".

```{r}
coords <- st_coordinates(philly.test) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

philly.test$lagPrice <- lag.listw(spatialWeights, philly.test$sale_price)

```


```{r}
coords.test <-  st_coordinates(philly.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")

ggplot() +
  geom_point(data = philly.test, aes(x = lagPrice, y = sale_price), size = 2) +
  geom_smooth(data = philly.test, aes(x = lagPrice, y = sale_price), method = "lm", se = F, colour = "#FA7800") +
  labs(title = "Price as a function of the spatial lag of price",
       x = "Spatial Lag of Price (Mean price of 5 nearest neighbors)",
       y = "Sale Price")

philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot()+
  geom_point(aes(x =lagPriceError, y =sale_price.Error)) +
  geom_smooth(aes(x = lagPriceError, y = sale_price.Error),method = "lm", se=F, colour = "#FA7800") +
  labs(title = "Error as a function of the spatial lag of price",
       x = "Spatial Lag of Error (Mean error of 5 nearest neighbors)", y = "Sale Price") 

```

## Do Errors Cluster? Using Moran's I

So - is your Moran's I statistic indicating dispersion (-1), randomness (0) or clustering (1)?


```{r}
moranTest <- moran.mc(philly.test$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()

```

## Predictions by neighborhood

```{r}
philly.test %>%
as.data.frame() %>%
# Tract is not a good factor here
  group_by(zip_code) %>%
    summarize(meanPrediction = mean(sale_price.Predict),
              meanPrice = mean(sale_price)) %>%
      kable() %>% 
  kable_styling()

```

## Regression with neighborhood effects

Let's try to run the regression again, but this time with a neighborhood fixed effect

```{r}
reg.nhood <- lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                                 dplyr::select(zip_code, sale_price,
                                              total_livable_area, crimes.Buffer))

philly.test.nhood <-
  philly.test %>%
  mutate(Regression = "Neighborhood Effects",
         sale_price.Predict = predict(reg.nhood, philly.test),
         sale_price.Error = sale_price.Predict- sale_price,
         sale_price.AbsError = abs(sale_price.Predict- sale_price),
         sale_price.APE = (abs(sale_price.Predict- sale_price)) / sale_price)%>%
  filter(sale_price < 5000000)

```

How do these models compare? We can bind our error info together and then examine!

```{r}
bothRegressions <- 
  rbind(
    dplyr::select(philly.test, starts_with("sale_price"), Regression, zip_code) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)),
    dplyr::select(philly.test.nhood, starts_with("sale_price"), Regression, zip_code) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)))  

```

Why do these values differ from those in the book?

```{r}
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -zip_code) %>%
  filter(Variable == "sale_price.AbsError" | Variable == "sale_price.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable()
```

## Further examination of errors

Predicted versus observed plots - what does it mean if the line is above or below y=x?

```{r}
bothRegressions %>%
  dplyr::select(sale_price.Predict, sale_price, Regression) %>%
    ggplot(aes(sale_price, sale_price.Predict)) +
  geom_point() +
  stat_smooth(aes(sale_price, sale_price.Predict), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(sale_price.Predict, sale_price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme()

```

We can also examine the spatial pattern of errors.

```{r}

st_drop_geometry(bothRegressions) %>%
  group_by(Regression, zip_code) %>%
  summarize(mean.MAPE = mean(sale_price.APE, na.rm = T)) %>%
  ungroup() %>% 
  st_join(acsTractsPHL.2020) %>%
    st_sf() %>%
    ggplot() + 
      geom_sf(aes(fill = mean.MAPE)) +
      geom_sf(data = bothRegressions, colour = "black", size = .1) +
      facet_wrap(~Regression) +
      scale_fill_gradient(low = palette5[1], high = palette5[5],
                          name = "MAPE") +
      labs(title = "Mean test set MAPE by neighborhood") +
      mapTheme()

```

## Race and income context of predictions

What is the race and income context of Boston census tracts, and how does this relate to our model performance?

```{r}
tracts20 <- 
  get_acs(geography = "tract", variables = c("B25026_001E","B02001_002E","B19013_001E"), 
          year=2020, state=42, county=101, geometry=TRUE, output="wide") %>%
  st_transform('ESRI:102729') %>% 
  rename(TotalPop = B25026_001E,
         NumberWhites =B02001_002E,
         Median_Income = B19013_001E) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))
  
grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
    labs(title = "Race Context") +
    mapTheme() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(tracts20), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Income Context") +
    mapTheme() + 
    theme(legend.position="bottom"))

```


```{r}

st_join(bothRegressions, tracts20) %>% 
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Test set MAPE by neighborhood racial context")

```
111
